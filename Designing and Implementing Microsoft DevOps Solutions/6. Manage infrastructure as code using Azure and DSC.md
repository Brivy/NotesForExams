# Manage infrastructure as code using Azure and DSC

IaC.

## Explore infrastructure as code and configuration management

Describes key concepts of infrastructure as code and environment deployment creation and configuration. Also, understand the imperative, declarative, and idempotent configuration and how it applies to your company.

### Explore environment deployment

The following table lists the significant differences between manual deployment and infrastructure as code:

| Manual deployment                             | Infrastructure as code                              |
|-----------------------------------------------|-----------------------------------------------------|
| Snowflake servers.                            | A consistent server between environments.           |
| Deployment steps vary by environment.         | Environments are created or scaled easily.          |
| More verification steps and more elaborate manual processes. | Fully automated creation of environment updates. |
| Increased documentation to account for differences. | Transition to immutable infrastructure.        |
| Deployment on weekends to allow time to recover from errors. | Use blue/green deployments.           |
| Slower release cadence to minimize pain and long weekends. | Treat servers as cattle, not pets.        |

And here is a great list of benefits:

* Promotes auditing by making it easier to trace what was deployed, when, and how. (In other words, it improves traceability.)
* Provides consistent environments from release to release.
* Greater consistency across development, test, and production environments.
* Automates scale-up and scale-out processes.
* Allows configurations to be version-controlled.
* Provides code review and unit-testing capabilities to help manage infrastructure changes.
* Uses immutable service processes, meaning if a change is needed to an environment, a new service is deployed, and the old one was taken down; it isn't updated.
* Allows blue/green (A/B) deployments.
* Treats infrastructure as a flexible resource that can be provisioned, de-provisioned, and reprovisioned as and when needed.

### Examine environment configuration

**Configuration management** refers to automated configuration management, typically in version-controlled scripts, for an application and all the environments needed to support it. Here are some differences between manual configuration and configuration as code:

| Manual configuration                         | Configuration as code                             |
|-----------------------------------------------|---------------------------------------------------|
| Configuration bugs are challenging to identify. | Bugs are easily reproducible.                  |
| Error-prone.                                  | Consistent configuration.                      |
| More verification steps and more elaborate manual processes. | Increase deployment cadence to reduce the amount of incremental change. |
| Increased documentation.                      | Treat environment and configuration as executable documentation. |
| Deployment on weekends to allow time to recover from errors. |  |
| Slower release cadence to minimize the requirement for long weekends. |  |

And again, some benefits:

* Bugs are more easily reproduced, audit help, and improve traceability.
* Provides consistency across environments such as dev, test, and release.
* It increased deployment cadence.
* Less documentation is needed and needs to be maintained as all configuration is available in scripts.
* Enables automated scale-up and scale out.
* Allows version-controlled configuration.
* Helps detect and correct configuration drift.
* Provides code-review and unit-testing capabilities to help manage infrastructure changes.
* Treats infrastructure as a flexible resource.
* Promotes automation.

### Understand imperative versus declarative configuration

There are a few different approaches that you can adopt to implement Infrastructure as Code and Configuration as Code:

* **Declarative** (functional). The declarative approach states what the final state should be. When run, the script or definition will initialize or configure the machine to have the finished state declared without defining how that final state should be achieved.

  ![Cool picture](Pictures/declarative-703bb981.png)

* **Imperative** (procedural). In the imperative approach, the script states the how for the final state of the machine by executing the steps to get to the finished state. It defines what the final state needs to be but also includes how to achieve that final state. It also can consist of coding concepts such as for, *if-then, loops, and matrices.

  ![Cool picture](Pictures/imperative-a9e6a2ad.png)

A declarative approach would *generally* be the preferred option where ease of use is the primary goal. Azure Resource Manager template files are an example of a declarative automation approach.

An imperative approach may have some advantages in complex scenarios where changes in the environment occur relatively frequently, which need to be accounted for in your code.

### Understand idempotent configuration

**Idempotence** is a mathematical term that can be used in Infrastructure as Code and Configuration as Code. It can apply one or more operations against a resource, resulting in the same outcome.

You can achieve idempotency by:

* Automatically configuring and reconfiguring an existing set of resources.
* Discarding the existing resources and recreating a new environment.

## Create Azure resources using Azure Resource Manager templates

Using Azure Resource Manager templates and their components and details dependencies and modularized templates with secrets.

### Why use Azure Resource Manager templates?

Using **Resource Manager templates** will make your deployments faster and more repeatable. Some benefits:

* Templates **improve consistency**. Resource Manager templates provide a common language for you and others to describe your deployments.
* Templates help **express complex deployments**. Resource Manager maps out each resource and its dependent resources and creates dependent resources first. Dependency mapping helps ensure that the deployment is carried out in the correct order.
* Templates **reduce manual, error-prone tasks**.
* Templates are **code**. The template code documents the deployment. Also, most users maintain their templates under revision control, such as GIT.
* Templates **promote reuse**. Your template can contain parameters that are filled in when the template runs. They also enable you to create multiple versions of your infrastructure, such as staging and production, while still using the same template.
* Templates are **linkable**. You can link Resource Manager templates together to make the templates themselves modular. You can write small templates that define a solution and then combine them to create a complete system.

### Explore template components

A Resource Manager template can contain sections that are expressed using JSON notation but aren't related to the JSON language itself:

**Parameters** are where you specify which values are configurable when the template runs. Here's an example that illustrates two parameters: one for a virtual machine's (VMs) username and one for its password:

```JSON
"parameters": {
  "adminUsername": {
    "type": "string",
    "metadata": {
      "description": "Username for the Virtual Machine."
    }
  },
  "adminPassword": {
    "type": "securestring",
    "metadata": {
      "description": "Password for the Virtual Machine."
    }
  }
}
```

**Variables** are where you define values that are used throughout the template. Here's an example that illustrates a few variables that describe networking features for a VM:

```JSON
"variables": {
  "nicName": "myVMNic",
  "addressPrefix": "10.0.0.0/16",
  "subnetName": "Subnet",
  "subnetPrefix": "10.0.0.0/24",
  "publicIPAddressName": "myPublicIP",
  "virtualNetworkName": "MyVNET"
}
```

**Functions** are where you define procedures that you don't want to repeat throughout the template. Here's an example that creates a function for creating a unique name to use when creating resources that have globally unique naming requirements:

```JSON
"functions": [
  {
    "namespace": "contoso",
    "members": {
      "uniqueName": {
        "parameters": [
          {
            "name": "namePrefix",
            "type": "string"
          }
        ],
        "output": {
          "type": "string",
          "value": "[concat(toLower(parameters('namePrefix')), uniqueString(resourceGroup().id))]"
        }
      }
    }
  }
],
```

**Resources** are where you define the Azure resources that make up your deployment. Here's an example that creates a public IP address resource:

```JSON
{
  "type": "Microsoft.Network/publicIPAddresses",
  "name": "[variables('publicIPAddressName')]",
  "location": "[parameters('location')]",
  "apiVersion": "2018-08-01",
  "properties": {
    "publicIPAllocationMethod": "Dynamic",
    "dnsSettings": {
      "domainNameLabel": "[parameters('dnsLabelPrefix')]"
    }
  }
}
```

**Outputs** are where you define any information you'd like to receive when the template runs. Here's an example that illustrates an output named hostname. The FQDN value is read from the VM's public IP address settings:

```JSON
"outputs": {
  "hostname": {
    "type": "string",
    "value": "[reference(variables('publicIPAddressName')).dnsSettings.fqdn]"
  }
}
```

### Manage dependencies

For any given resource, other resources might need to exist before you can deploy the resource. You define a dependency with the **dependsOn** element or by using the **reference** function. It will validate if this is possible or that you have a **circular dependency**. If so, it can't deploy the resources.

The *dependsOn* value can be a comma-separated list of resource names.

![Cool picture](Pictures/dependson-e5a30e56.png)

Some notes:

* You only need to define dependencies for resources that are deployed in the same template.
* When resources aren't dependent on each other, the Resource Manager deploys them in parallel.

### Modularize templates

When using Azure Resource Manager templates, it's best to modularize them by breaking them into individual components. The primary methodology to use is by using **linked templates**:

```JSON
"resources": [
  {
      "apiVersion": "2017-05-10",
      "name": "linkedTemplate",
      "type": "Microsoft.Resources/deployments",
      "properties": {
          "mode": "Incremental",
          <link-to-external-template>
      }
  }
]
```

Then there is also the **nested template**, where you can nest the template within the main template, use the template property, and specify the template syntax.

```JSON
"resources": [
  {
    "apiVersion": "2017-05-10",
    "name": "nestedTemplate",
    "type": "Microsoft.Resources/deployments",
    "properties": {
      "mode": "Incremental",
      "template": {
        "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
        "contentVersion": "1.0.0.0",
        "resources": [
          {
            "type": "Microsoft.Storage/storageAccounts",
            "name": "[variables('storageName')]",
            "apiVersion": "2015-06-15",
            "location": "West US",
            "properties": {
              "accountType": "Standard_LRS"
            }
          }
        ]
      }
    }
  }
]
```

**Note** that in nested templates, you can't use parameters or variables defined within the nested template itself for nested templates. You can only use parameters and variables from the main template.

### Deploying templates

When **deploying** your resources using templates, you have three options:

* **validate**. This option compiles the templates, validates the deployment, ensures the template is functional (for example, no circular dependencies), and correct syntax.
* **incremental mode** (default). This option only deploys whatever is defined in the template. It doesn't remove or modify any resources that aren't defined in the template (in comparison to "complete mode").
* **complete mode**: Resource Manager deletes resources that exist in the resource group but isn't specified in the template. For example, only resources defined in the template will be present in the resource group after the template deploys. As a best practice, use this mode for production environments to achieve idempotency in your deployment templates.

More notes:

* As a best practice, use one resource group per deployment.
* You can only use `incremental` deployment mode for both linked and nested templates.

### External template and external parameters

To link to an external template and parameter file, use **templateLink** and **parametersLink**. Some thoughts:

* When linking to a template, ensure that the Resource Manager service can access it.
* You can only provide a Uniform Resource Identifier (URI) value that includes HTTP or HTTPS.
* One option is to place your linked template in a storage account and use the URI for that item.
* To make the template private, you can add your template to a private storage account accessible to only the storage account owner, creating shared access signature (SAS) tokens to enable access during deployment.

It will look something like this:

```JSON
"resources": [
    {
      "name": "linkedTemplate",
      "type": "Microsoft.Resources/deployments",
      "apiVersion": "2018-05-01",
      "properties": {
          "mode": "Incremental",
          "templateLink": {
              "uri":"https://linkedtemplateek1store.blob.core.windows.net/linkedtemplates/linkedStorageAccount.json?sv=2018-03-28&sr=b&sig=dO9p7XnbhGq56BO%2BSW3o9tX7E2WUdIk%2BpF1MTK2eFfs%3D&se=2018-12-31T14%3A32%3A29Z&sp=r"
          },
          "parameters": {
              "storageAccountName":{"value": "[variables('storageAccountName')]"},
              "location":{"value": "[parameters('location')]"}
          }
      }
    },
```

### Manage secrets in templates

When passing a secure value (such as a password) as a parameter during deployment, you can retrieve the value from an Azure Key Vault. Note that the Key Vault can exist in a different subscription than the resource group you're deploying it to.

So were do we start? First we create a Key Vault via an Azure CLI script:

```PowerShell
keyVaultName='{your-unique-vault-name}'
resourceGroupName='{your-resource-group-name}'
location='centralus'
userPrincipalName='{your-email-address-associated-with-your-subscription}'

# Create a resource group
az group create --name $resourceGroupName --location $location

# Create a Key Vault
az keyvault create \
  --name $keyVaultName \
  --resource-group $resourceGroupName \
  --location $location \
  --enabled-for-template-deployment true
az keyvault set-policy --upn $userPrincipalName --name $keyVaultName --secret-permissions set delete get list

# Create a secret with the name, examplesecret
password=$(openssl rand -base64 32)
echo $password
az keyvault secret set --vault-name $keyVaultName --name 'examplesecret' --value $password
```

And before we update our Azure Resource Manager template, we have to set the the Key Vault property **enabledForTemplateDeployment** to **true**, and the user deploying the template must have `Microsoft.KeyVault/vaults/deploy/`action permission for the scope that contains the Key Vault.

Now that that's in place, we can use for the administrator password for a SQL database:

```JSON
{
  "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "adminLogin": {
      "type": "string"
    },
    "adminPassword": {
      "type": "securestring"
    },
    "sqlServerName": {
      "type": "string"
    }
  },
  "resources": [
    {
      "name": "[parameters('sqlServerName')]",
      "type": "Microsoft.Sql/servers",
      "apiVersion": "2015-05-01-preview",
      "location": "[resourceGroup().location]",
      "tags": {},
      "properties": {
        "administratorLogin": "[parameters('adminLogin')]",
        "administratorLoginPassword": "[parameters('adminPassword')]",
        "version": "12.0"
      }
    }
  ],
  "outputs": {
  }
}
```

Now we only need to create the parameters and reference the secret from the Key Vault:

```JSON
{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "adminLogin": {
            "value": "exampleadmin"
        },
        "adminPassword": {
            "reference": {
              "keyVault": {
                "id": "/subscriptions/<subscription-id>/resourceGroups/examplegroup/providers/Microsoft.KeyVault/vaults/<vault-name>"
              },
              "secretName": "examplesecret"
            }
        },
        "sqlServerName": {
            "value": "<your-server-name>"
        }
    }
}
```

## Create Azure resources by using Azure CLI

Using Azure CLI to create Azure resources, run templates, and detail Azure CLI commands.

### What is Azure CLI?

Azure CLI is a command-line program you use to connect to Azure and execute administrative commands on Azure resources. It runs on Linux, macOS, and Windows operating systems.

You can use Azure CLI interactively or through scripts:

* **Interactive**. For Windows operating systems, launch a shell such as cmd.exe, or for Linux or macOS, use Bash. Then issue the command at the shell prompt.
* **Scripted**. Assemble the Azure CLI commands into a shell script using the script syntax of your chosen shell, and then execute the script.

### Work with Azure CLI

Commands in the CLI are structured in **groups** and **subgroups**. Each group represents a service provided by Azure, and the subgroups divide commands for these services into logical groupings.

So, how do you find the commands you need? One way is to use the `az find` command.

Some cool commands you'll find along the way:

* Creating groups:

  ```Azure CLI
  az group create --name <name> --location <location>
  ```

* List your groups as a table:

  ```Azure CLI
  az group list --output table
  ```

* List your groups by querying them (you format the query using **JMESPath**, which is a standard query language for JSON requests.):

  ```Azure CLI
  az group list --query "[?name == '<rg name>']"
  ```

One more note, please be aware of any issues around the shell or environment you use to run the script. PowerShell and the Azure CLI are two different types of shells and have different ways of writing stuff.

## Explore Azure Automation with DevOps

Automate stuff in Azure.

### What is Azure Automation

Azure Automation is an Azure service that provides a way for users to automate the manual, long-running, error-prone, and frequently repeated tasks commonly done in a cloud and enterprise environment. You can automate processes using runbooks or automate configuration management by using **Desired State Configuration (DSC)**.

You can also use open-source tools to do some of these operations. However, the integration hooks available to Azure Automation remove much of the integration complexity you would have to manage if you did these operations manually.

Some Azure Automation capabilities are:

* **Process automation**. Azure Automation provides you with the ability to automate frequent, time-consuming, and error-prone cloud management tasks.
* **Azure Automation State Configuration**. It's an Azure service that allows you to write, manage, and compile Windows PowerShell DSC configurations, import DSC Resources, and assign configurations to target nodes, all in the cloud.
* **Update management**. Manage operating system updates for Windows and Linux computers in Azure, on-premises environments, or other cloud providers. Get update compliance visibility across Azure, on-premises, and for other cloud services. You can create scheduled deployments to orchestrate update installations within a defined maintenance window.
* **Start and stop virtual machines (VMs)**. Azure Automation provides an integrated Start/Stop VM–related resource that enables you to start and stop VMs on user-defined schedules. It also provides insights through Azure Log Analytics and can send emails by using action groups.
* **Integration with GitHub, Azure DevOps, Git, or Team Foundation Version Control repositories**.
* **Automate Amazon Web Services (AWS) Resources**. Automate common tasks with resources in AWS using Automation runbooks in Azure.
* **Manage Shared resources**. Azure Automation consists of a set of shared resources (such as connections, credentials, modules, schedules, and variables) that make it easier to automate and configure your environments at scale.
* **Run backups**. Azure Automation allows you to run regular backups of non-database systems, such as backing up Azure Blob Storage at certain intervals.

Azure Automation works across hybrid cloud environments in addition to Windows and Linux operating systems.

### Create automation accounts

To start using the Microsoft Azure Automation service, you must first create an **Automation account** from within the Azure portal. Automation accounts are like Azure Storage accounts in that they serve as a container to store automation artifacts like a container for all your runbooks, runbook executions (jobs), and the assets on which your runbooks depend.

An Automation account gives you access to managing all Azure resources via an API. To safeguard it, the Automation account creation requires subscription-owner access. You must be a subscription owner to create the Run As accounts that the service creates.

### What is a runbook?

**Runbooks** serve as repositories for your custom scripts and workflows. They also typically reference Automation shared resources such as credentials, variables, connections, and certificates. They can also contain other runbooks, allowing you to build more complex workflows. You can invoke and run runbooks either on-demand or according to a schedule by using **Automation Schedule assets**.

### Creating runbooks

When creating runbooks, you have two options. You can either:

* Create your runbook and import it.
* Modify runbooks from the runbook gallery. It provides a rich ecosystem of runbooks that are available for your requirements.

There's also a vibrant open-source community that creates runbooks you can apply directly to your use cases.

![Cool picture](Pictures/create-runbook-81c5672e.png)

Some of the runbook types:

* **Graphical runbooks**: Graphical runbooks and Graphical PowerShell Workflow runbooks are created and edited with the graphic editor in the Azure portal. You can export them to a file and import them into another automation account, but you can't create or edit them with another tool.
* **PowerShell runbooks**: PowerShell runbooks are based on Windows PowerShell. You can edit them in the Azure Portal or use any offline text editor and then import the runbook into Azure Automation. PowerShell runbooks don't use *parallel processing*.
* **PowerShell Workflow runbooks**: PowerShell Workflow runbooks are text runbooks based on Windows PowerShell Workflow. You can edit them in the Azure Portal or use any offline text editor and then import the runbook into Azure Automation. They use *parallel processing* to allow for simultaneous completion of multiple tasks, but they also take longer to start than PowerShell runbooks because they must be compiled before running.
* **Python runbooks**: Python runbooks compile under Python 2. You can edit them in the Azure Portal or use any offline text editor and then import the runbook into Azure Automation. You can also use Python libraries. However, only Python 2 is supported at this time. To use third-party libraries, you must first import the package into the Automation Account.

Important takeaway from this: You can't convert runbooks from graphical to textual type, and the other way around.

### Understand automation shared resources

Azure Automation contains shared resources that are globally available to be associated with or used in a runbook. There are currently eight shared resources categories:

* **Schedules**: It allows you to define a one-off or recurring schedule.
* **Modules**: Contains Azure PowerShell modules.
* **Modules gallery**: It allows you to identify and import PowerShell modules into your Azure automation account.
* **Python packages**. Allows you to import a Python package by uploading: .whl or tar.gz packages.
* **Credentials**: It allows you to create username and password credentials.
* **Connections**: It allows you to specify Azure, Azure classic certificate, or Azure Service principal connections.
* **Certificates**: It allows you to upload certificates in .cer or pfx format.
* **Variables**: It allows you to define encrypted or unencrypted variables of types—for example, String, Boolean, DateTime, Integer, or no specific type.

![Cool picture](Pictures/shared-resources-4e336a1e.png)

### Explore runbook gallery

In the Azure portal, you can import directly from the runbook gallery using the following high-level steps:

1. Open your Automation account, and then select *Process Automation > Runbooks*.
2. In the runbooks pane, select *Browse gallery*.
3. From the runbook gallery, locate the runbook item you want, select it, and select Import.

When browsing through the runbooks in the repository, you can review the code or visualize the code. You can also check information such as the source project and a detailed description, ratings, and questions and answers.

![Cool picture](Pictures/runbook-gallery-azure-automation-70bd55e6.png)

Some notes:

* Only the new Azure PowerShell (**Az**, released in 2018) is supported.
* Python runbooks are also available from the Azure Automation Github in the runbooks repository. To find them, filter by language and select Python.
* You can't use PowerShell to import directly from the runbook gallery.

### Examine webhooks

You can automate starting a runbook either by scheduling it or by using a **webhook**.This allows you to start a particular runbook in Azure Automation through a single HTTPS request.

![Cool picture](Pictures/1-5-4-webhook-runbook-e6c98334.png)

You create a webhook linked to a runbook using the following steps:

1. In the Azure portal, open the runbook that you want to create the webhook.
2. In the runbook pane, under Resources, select Webhooks, and then choose + Add webhook.
3. Select Create new webhook.
4. In the Create new webhook dialog, there are several values you need to configure. After you configure them, select Create:
    * **Name**. Specify any name you want for a webhook because the name isn't exposed to the client. It's only used for you to identify the runbook in Azure Automation.
    * **Enabled**. A webhook is enabled by default when it's created. If you set it to Disabled, then no client can use it.
    * **Expires**. Each webhook has an expiration date, at which time it can no longer be used. You can continue to modify the date after creating the webhook providing the webhook isn't expired.
    * **URL**. The webhook URL is the unique address that a client calls with an HTTP POST to start the runbook linked to the webhook. It's automatically generated when you create the webhook, and you can't specify a custom URL. The URL contains a security token that allows the runbook to be invoked by a third-party system with no further authentication. For this reason, treat it like a password. You can only view the URL in the Azure portal for security reasons when the webhook is created. Make a note of the URL in a secure location for future use, because **you can't retrieve the URL again**.
5. Select the **Parameters run settings (Default: Azure)** option. This option has the following characteristics, which allows you to complete the following actions:
    * If the runbook has mandatory parameters, you'll need to provide these required parameters during creation. You aren't able to create the webhook unless values are provided.
    * If there are no mandatory parameters in the runbook, there's no configuration required here.
    * The webhook must include values for any mandatory parameters of the runbook and include values for optional parameters.
    * When a client starts a runbook using a webhook, it can't override the parameter values defined.
    * To receive data from the client, the runbook can accept a single parameter called $WebhookData of type [object] that contains data that the client includes in the POST request.
    * There's no required webhook configuration to support the $WebhookData parameter.
6. When finished, select Create.

![Cool picture](Pictures/create-webhook-e62bc51c.png)

Now that we have our webhook, your client application must issue an HTTP POST with the URL for the webhook.

```HTTP
http://< Webhook Server >/token?=< Token Value >
```

If successful, the webhook response contains the job ID in JSON format as follows:

```JSON
{"JobIds":["< JobId >"]}
```

ou can't determine when the runbook job completes or determine its completion status from the webhook. You can only choose this information using the job ID with another method such as PowerShell or the Azure Automation API.

### Explore source control integration

Azure Automation supports source control integration that enables you to keep your runbooks in your Automation account up to date with your scripts in your source control repository. It supports three types of source control:

* GitHub.
* Azure DevOps (Git).
* Azure DevOps (TFVC).

To use this integration, follow the next steps:

1. In the Azure portal, access your Automation account.
2. Under Account Settings, select Source control, and then choose + Add.
3. In the Source Control Summary blade, select GitHub as source control type and then select Authenticate.
4. When the browser page opens, prompting you to authenticate to `https://www.github.com`, select **Authorize azureautomation** and enter your GitHub account password.
5. After authentication completes, fill in the details and then select Save:
    * Name: Friendly name
    * Source control type: GitHub, Azure DevOps Git, or Azure DevOps TFVC
    * Repository: The name of the repository or project
    * Branch: The branch from which to pull the source files. Branch targeting isn't available for the TFVC source control type.
    * Folder Path: The folder that contains the runbooks to sync.
    * Autosync: Turns on or off automatic sync when a commit is made in the source control repository.
    * Publish Runbook: If set to On, after runbooks are synced from source control, they'll be automatically published.
    * Description: A text field to provide more details.
6. If you set Autosync to Yes, full sync will start. If you set Autosync to No, open the Source Control Summary blade again by selecting your repository in Azure Automation and then selecting Start Sync.
7. Verify that your source control is listed on the Azure Automation Source control page for you to use.

![Cool picture](Pictures/source-control-summary-4b83dab3.png)

### Explore PowerShell workflows

PowerShell Workflow lets IT pros and developers apply the benefits of Windows Workflow Foundation with the automation capabilities and ease of using Windows PowerShell. A workflow can:

* Be long-running.
* Be repeated over and over.
* Run tasks in parallel.
* Be interrupted—can be stopped and restarted, suspended, and resumed.
* Continue after an unexpected interruption, such as a network outage or computer/server restart.

It consists out of **activities**. This is a specific task that you want a workflow to do. Just as a script is composed of one or more commands, a workflow is composed of activities carried out in sequence.

A workflow offers many benefits, including:

* Windows PowerShell scripting syntax. Is built on PowerShell.
* Multidevice management. Simultaneously apply workflow tasks to hundreds of managed nodes.
* Single task runs multiple scripts and commands. Combine related scripts and commands into a single task. Then run the single task on multiple computes. The activity status and progress within the workflow are visible at any time.
* Automated failure recovery. For example, it survives both planned and unplanned interruptions, such as computer restarts.
* Connection and activity retries.
* Connect and disconnect from workflows. Users can connect and disconnect from the computer running the workflow, but the workflow will remain running.
* Task scheduling.

### Create a workflow

Use a script editor such as the Windows PowerShell Integrated Scripting Environment (ISE) to write the workflow. Because the syntactic differences between scripts and workflows are significant, a tool that knows both workflows and scripts will save you considerable coding and testing time.

The syntax is as follows:

1. A workflow is a Windows command type, so select a name with a verb-noun format:

    ```PowerShell
    workflow Test-Workflow
      {
          ...
      }
    ```

2. To add parameters to a workflow, use the `Param` keyword. It's the same techniques that you use to add parameters to a function.
3. Add your standard PowerShell commands.

    ```PowerShell
    workflow MyFirstRunbook-Workflow
      {
          Param(
                [string]$VMName,
                [string]$ResourceGroupName
            )
            ....
          Start-AzureRmVM -Name $VMName -ResourceGroupName $ResourceGroupName
      }
    ```

### Examine checkpoint and parallel processing

Workflows let you implement complex logic within your code. Two features available with workflows are **checkpoints** and **parallel processing**.

A checkpoint is a *snapshot* of the current state of the workflow. Checkpoints include the current value for variables and any output generated up to that point. If a workflow ends in an error or is suspended, the next time it runs, it will start from its last checkpoint instead of at the beginning of the workflow.

```PowerShell
  <Activity1>
    Checkpoint-Workflow
      <Activity2>
        <Exception>
      <Activity3>
```

A script block has multiple commands that run concurrently (or in parallel) instead of sequentially, as for a typical script.

```PowerShell
Parallel
{
    Start-AzureRmVM -Name $vm0 -ResourceGroupName $rg 
    Start-AzureRmVM -Name $vm1 -ResourceGroupName $rg
}

Start-AzureRmVM -Name $vm2 -ResourceGroupName $rg
```

You can use the `ForEach -Parallel` construct to concurrently process commands for each item in a collection. The items in the collection are processed in parallel while the commands in the script block run sequentially.

One extra thing: We use the `-ThrottleLimit` parameter to limit parallelism.

```PowerShell
ForEach -Parallel -ThrottleLimit 10 ($<item> in $<collection>)
{
    <Activity1>
    <Activity2>
}
<Activity3>
```

A real-world example of it could be similar to the following code: a message displays for each file after it's copied. Only after all files are copied does the completion message display:

```PowerShell
Workflow Copy-Files
{
    $files = @("C:\LocalPath\File1.txt","C:\LocalPath\File2.txt","C:\LocalPath\File3.txt")
   
    ForEach -Parallel -ThrottleLimit 10 ($File in $Files)
    {
        Copy-Item -Path $File -Destination \\NetworkPath
        Write-Output "$File copied."
    }
  
    Write-Output "All files copied."
}
```

## Implement Desired State Configuration (DSC)

Desired State Configuration (DSC) and its components for implementation.

### Understand configuration drift

**Configuration drift** is the process of a set of resources changing over time from their original deployment state. Eventually, an environment can become a *snowflake*. That is a unique configuration that cannot be reproduced automatically and is typically a result of configuration drift.

Configuration drift can also introduce security vulnerabilities into your environment. For example:

* Ports might be opened that should be kept closed.
* Updates and security patches might not be applied across environments consistently.
* The software might be installed that doesn't meet compliance requirements.

there are many ways you can manage it in your environments using configuration management tools and products such as:

* Windows PowerShell Desired State Configuration. It's a management platform in PowerShell that enables you to manage and enforce resource configurations.
* Azure Policy. Use Azure Policy to enforce policies and compliance standards for Azure resources.

### Explore Desired State Configuration (DSC)

**Desired State Configuration (DSC)** is a configuration management approach that you can use for configuration, deployment, and management of systems to ensure that an environment is maintained in a state that you specify (defined state) and doesn't deviate from that state.

DSC consists of three primary parts:

* **Configurations**. These are declarative PowerShell scripts that define and configure instances of resources. Upon running the configuration, DSC (and the resources being called by the configuration) will apply the configuration, ensuring that the system exists in the state laid out by the configuration. DSC configurations are also idempotent: The Local Configuration Manager (LCM) will ensure that machines are configured in whatever state the configuration declares.
* **Resources**. They contain the code that puts and keeps the target of a configuration in the specified state. Resources are in PowerShell modules and can be written to a model as generic as a file or a Windows process or as specific as a Microsoft Internet Information Services (IIS) server or a VM running in Azure.
* **Local Configuration Manager (LCM)**. The LCM runs on the nodes or machines you wish to configure. It's the engine by which DSC facilitates the interaction between resources and configurations. The LCM regularly polls the system using the control flow implemented by resources to maintain the state defined by a configuration. If the system is out of state, the LCM calls the code in resources to apply the configuration according to specified.

There are two methods of implementing DSC:

* **Push mode**: A user actively applies a configuration to a target node and pushes out the configuration.
* **Pull mode**: is where pull clients are automatically configured to get their desired state configurations from a remote pull service. This remote pull service is provided by a pull server that acts as central control and manager for the configurations, ensures that nodes conform to the desired state, and reports on their compliance status.

![Cool picture](Pictures/dsc2-6e218006.png)

### Explore Azure Automation State configuration (DSC)

**Azure Automation State configuration DSC** is an Azure cloud-based implementation of PowerShell DSC, available as part of Azure Automation. Azure Automation State configuration allows you to write, manage, and compile PowerShell DSC configurations, import DSC Resources, and assign configurations to target nodes, all in the cloud. It has:

* *Built-in pull server*. Provides a DSC pull server like the Windows Feature DSC service so that target nodes automatically receive configurations, conform to the desired state, and report back on their compliance. The built-in pull server in Azure Automation eliminates the need to set up and maintain your pull server.
* *Management of all your DSC artifacts*. You can manage all your DSC configurations, resources, and target nodes from the Azure portal or PowerShell.
* *Import reporting data into Log Analytics*. Nodes managed with Azure Automation state configuration send detailed reporting status data to the built-in pull server. You can configure Azure Automation state configuration to send this data to your Log Analytics workspace.

It works as follows:

1. Create a PowerShell script with the configuration element.
2. Upload the script to Azure Automation and compile the script into a MOF (Managed Object Format) file. The file is transferred to the DSC pull server, provided as part of the Azure Automation service.
3. Define the nodes that will use the configuration, and then apply the configuration.

![Cool picture](Pictures/dsc1-127af77c.png)

### Examine DSC configuration file

**DSC configurations** are Windows PowerShell scripts that define a special type of function.

```PowerShell
configuration LabConfig
{
    Node WebServer
    {
        WindowsFeature IIS
        {
            Ensure = 'Present'
            Name = 'Web-Server'
            IncludeAllSubFeature = $true
        }
    }
}
```

Code in more details:

* **Configuration block**. The `Configuration` block is the outermost script block. In this case, the name of the configuration is `LabConfig`.
* **Node block**. There can be one or more `Node` blocks. It defines the nodes (computers and VMs) that you're configuring. In this example, the node targets a computer called `WebServer`. You could also call it `localhost` and use it locally on any server.
* **Resource blocks**. There can be one or more resource blocks. It's where the configuration sets the properties for the resources. In this case, there's one resource block called `WindowsFeature`.

A more complex example:

```PowerShell
Configuration MyDscConfiguration
{
    param
    (
        [string[]]$ComputerName='localhost'
    )

    Node $ComputerName
    {
        WindowsFeature MyFeatureInstance
        {
            Ensure = 'Present'
            Name = 'RSAT'
        }

        WindowsFeature My2ndFeatureInstance
        {
            Ensure = 'Present'
            Name = 'Bitlocker'
        }
    }
}
```

### Explore hybrid management

The **Hybrid Runbook Worker** feature of Azure Automation allows you to run runbooks that manage local resources in your private data center on machines located in your data center. Azure Automation stores and manages the runbooks and then delivers them to one or more on-premises machines.

![Cool picture](Pictures/dsc9-4ce36ece.png)

The following list is characteristics of the Hybrid Runbook Worker workflow:

* You can select one or more computers in your data center to act as a Hybrid Runbook Worker and then run runbooks from Azure Automation.
* Each Hybrid Runbook Worker is a member of a Hybrid Runbook Worker group, which you specify when you install the agent.
* A group can include a single agent, but you can install multiple agents in a group for high availability.
* There are no inbound firewall requirements to support Hybrid Runbook Workers, only Transmission Control Protocol (TCP) 443 is required for outbound internet access.
* The agent on the local computer starts all communication with Azure Automation in the cloud.
* When a runbook is started, Azure Automation creates an instruction that the agent retrieves. The agent then pulls down the runbook and any parameters before running it.

To configure your on-premises servers that support the Hybrid Runbook Worker role with DSC, you must add them as DSC nodes.

### Implement DSC and Linux Automation on Azure

More about that found [here](https://learn.microsoft.com/en-us/powershell/dsc/getting-started/lnxGettingStarted?view=dsc-1.1&viewFallbackFrom=powershell-7.3)

## Implement Bicep

What is Bicep?

### What is Bicep?

**Azure Bicep** is the next revision of **ARM templates** designed to solve some of the issues developers were facing when deploying their resources to Azure:

* It's open source.
* It's a domain-specific language (DSL), meaning it provides declaratively codify infrastructure, which describes the topology of cloud resources such as VMs, Web Apps, and networking interfaces.
* Encourages code reuse and modularity in designing the infrastructure as code files.
* New syntax to write less code compared to ARM templates.

Azure Bicep comes with its **command line interface (CLI)**, which can be used independently or with Azure CLI. Bicep CLI allows you to *transpile* the Bicep files into ARM templates and deploy them and can be used to convert an existing ARM template to Bicep.

### Exercising with the Bicep templates

In the example given, we create two parameters that we can use for our template:

```Bicep
param storageName string = 'stg${uniqueString(resourceGroup().id)}'
param location string = resourceGroup().location
```

The following code will use the parameters and deploy an Azure storage account:

```Bicep
resource storageaccount 'Microsoft.Storage/storageAccounts@2021-02-01' = {
    name: 'name'
    location: location
    kind: 'StorageV2'
    sku: {
        name: 'Premium_LRS'
    }
}
```

You can manually compile the file if you want:

```Bash
az bicep build -f ./main.bicep
```

But you don't need to if you want to deploy:

```Bash
az group create --name Bicep --location eastus
az deployment group create --resource-group Bicep --template-file main.bicep --parameters storageName=brianStorageAccount
```

### Understand Bicep file structure and syntax

Some main concepts will be discussed here:

```Bicep
@minLength(3)
@maxLength(11)
param storagePrefix string

param storageSKU string = 'Standard_LRS'
param location string = resourceGroup().location

var uniqueStorageName = '${storagePrefix}${uniqueString(resourceGroup().id)}'

resource stg 'Microsoft.Storage/storageAccounts@2019-04-01' = {
    name: uniqueStorageName
    location: location
    sku: {
        name: storageSKU
    }
    kind: 'StorageV2'
    properties: {
        supportsHttpsTrafficOnly: true
    }

    resource service 'fileServices' = {
        name: 'default'

        resource share 'shares' = {
            name: 'exampleshare'
        }
    }
}

module webModule './webApp.bicep' = {
    name: 'webDeploy'
    params: {
        skuName: 'S1'
        location: location
    }
}

output storageEndpoint object = stg.properties.primaryEndpoints
```

The scope of this code is default set to `resourceGroup`. You can set it explicitly to `subscription`, `managementGroup`, and `tenant`. Each section in this code has it's own meaning:

* **Parameters**: They allow you to customize your template deployment at run time by providing potential values. They also have types that editors can validate and also can have default values to make them optional at deployment time.
* **Variables**: Any complex expression can be stored in a variable and used throughout the template. When you define a variable, the type is inferred from the value.
* **Resources**: The resource keyword is used when you need to declare a resource in your templates. There are certain properties that are common for all resources such as `location`, `name`, and `properties`. There are resource-specific properties that can be used to customize the resource `pricing tier`, `SKU`, and so on. Furthermore, you can define subresources within a resource or outside by referencing the parent.
* **Modules**: Modules enable you to reuse a Bicep file in other Bicep files. In a module, you define what you need to deploy, and any parameters needed and when you reuse it in another file, all you need to do is reference the file and provide the parameters. The rest is taken care of by Azure Bicep.
* **Outputs**: Allow you to pass values from your deployment to the outside world, whether it is within a CI/CD pipeline or in a local terminal or Cloud Shell.
* **Other stuff**: There are many other features available within a Bicep file such as loops, conditional deployment, multiline strings, referencing an existing cloud resource, and many more. In fact, any valid function within an ARM template is also valid within a Bicep file.

### Deploying the Bicep template via Azure Pipelines

See this example:

```YAML
trigger:
    - main

name: Deploy Bicep files

variables:
    vmImageName: 'ubuntu-latest'

    azureServiceConnection: 'myServiceConnection'
    resourceGroupName: 'Bicep'
    location: 'eastus'
    templateFile: 'main.bicep'
pool:
    vmImage: $(vmImageName)

steps:
    - task: AzureCLI@2
      inputs:
          azureSubscription: $(azureServiceConnection)
          scriptType: bash
          scriptLocation: inlineScript
          inlineScript: |
            az --version
            az group create --name $(resourceGroupName) --location $(location)
            az deployment group create --resource-group $(resourceGroupName) --template-file $(templateFile)
```

Please note that:

* You need to have a configured service connection that is linked to your Azure subscription.
* You need to have that Bicep file you created earlier pushed into the Azure Repository of your project.

### Deploying the Bicep template via GitHub Actions

But before we do that, we need a service principal which GitHub can use:

```Bash
az login
az ad sp create-for-rbac --name myApp --role contributor --scopes /subscriptions/{subscription-id}/resourceGroups/Bicep --sdk-auth
```

We need to copy the output and use it into a new GitHub action secret called `AZURE_CREDENTIALS`. You may want to create other secrets here as well.

```YAML
on: [push]
name: Azure ARM
jobs:
    build-and-deploy:
        runs-on: ubuntu-latest
        steps:
            # Checkout code
            - uses: actions/checkout@main

              # Log into Azure
            - uses: azure/login@v1
              with:
                  creds: ${{ secrets.AZURE_CREDENTIALS }}

              # Deploy Bicep file
            - name: deploy
              uses: azure/arm-deploy@v1
              with:
                  subscriptionId: ${{ secrets.AZURE_SUBSCRIPTION }}
                  resourceGroupName: ${{ secrets.AZURE_RG }}
                  template: ./main.bicep
                  parameters: storagePrefix=stg
                  failOnStdErr: false
```

Note that you'll need to push the Bicep file into the GitHub repository.
