# Explore core data concepts

The first chapter in a this journey.

## Explore core data concepts (the chapter)

The capability to capture, store, and analyze data is a core requirement for every organization in the world. So better you learn about it.

### Identify data formats

Data comes in two forms:

- **Entities**: These are the names of data structures that are important for the organization (customers, products, etc.)
- **Attributes**: This is extra data that belongs to that entity (a customer his name, address, phone number, etc.)

You can also classify data. This comes in three types:

- **Structured data** adheres to a *fixed schema* (so all data has the same fields/properties). It's also *tabular* which means that an entity is represented over one or more tables with rows.
- **Semi-structured data** has some structure, but not every entity is the same (some customers have an email address, some don't). It's mostly written in JSON.
- **Unstructured data** is data that doesn't relate to each other at all (like documents, images, etc.).

### Explore file storage

While your basic computer also does file storage, data needs some more attention:

- What kind of data do we store (structure, semi-structured or unstructured).
- The applications and services that need to read, write and process the data.
- The need for the data files to be readable by humans or optimized for efficient storage and processing.

There are a couple data types that you should know of:

- **Delimited text files**. This is like storing data with specific field delimiters and row terminators. A couple examples of these are:
  - *CSV*, comma-separated values.
  - *TSV*, tab-separated values.
  - White space separated values.
- **JSON (JavaScript Object Notation)**. A ubiquitous format in which a hierarchical document schema is used to define data entities (objects) that have multiple attributes. Each attribute might be an object (or a collection of objects); making JSON a flexible format that's good for both structured and semi-structured data.
- **XML (Extensible Markup Language)**. The earlier version of JSON and human readable.
- **BLOB (Binary Large Object)**. While all files are eventually stored as binary data, but you can also save files directly to this. Mostly used for unstructured data.

There are also some special file formats, optimized for storing data (but not human readable):

- **Avro**. It is a good format for compressing data and minimizing storage and network bandwidth requirements. Each record contains a JSON header that describes the structure of the data in the record. The data is stored as binary information. An application uses the information in the header to parse the binary data and extract the fields it contains.
- **ORC (Optimized Row Columnar format)**. It contains stripes of data. Each stripe holds the data for a column or set of columns. A stripe contains an index into the rows in the stripe, the data for each row, and a footer that holds statistical information (count, sum, max, min, and so on) for each column.
- **Parquet**. It specializes in storing and processing nested data types efficiently. It supports very efficient compression and encoding schemes. Each row group contains one or more chunks of data. A Parquet file includes metadata that describes the set of rows found in each chunk. An application can use this metadata to quickly locate the correct chunk for a given set of rows, and retrieve the data in the specified columns for these rows.

### Explore databases

A database is a system in which data can be stored and queried. There are two kinds of databases:

- **Relational databases** are commonly used to store and query *structured data*. A table represents an entity and each instance has been given a *primary key* to identify the stored entity. The use of primary keys in other tables is called *foreign keys*. By using this keys, the database can be *normalized* which means the elimination of duplication.
- **Non-relational databases (NoSQL)** are used when data doesn't apply to a relation schema. There are actually for common types of these databases:
  - *Key-value databases*. Each record consists of a unique key and an associated value, which can be in any format.

    ![Cool picture](Pictures/key-value-store.png)
  - *Document databases*. A specific form of key-value database in which the value is a JSON document (which the system is optimized to parse and query).

    ![Cool picture](Pictures/document-store.png)
  - *Column family databases*. Stores tabular data comprising rows and columns, but you can divide the columns into groups known as column-families. Each column family holds a set of columns that are logically related together.

    ![Cool picture](Pictures/column-family-store.png)
  - *Graph databases*. Store entities as nodes with links to define relationships between them.

    ![Cool picture](Pictures/graph.png)

### Explore transactional data processing

A transactional system records *transactions* that encapsulate specific events that the organization wants to track. The work performed by transactional systems is often referred to as *Online Transactional Processing (OLTP)*. Such a system is performing CRUD operation which should be applied within a transaction. They must support ACID to accomplish this:

- **Atomicity**: each transaction is treated as a single unit, which success completely or fails completely.
- **Consistency**: transactions can only take the data in the database from one valid state to another.
- **Isolation**: concurrent transactions cannot interfere with one another, and must result in a consistent database state.
- **Durability**: when a transaction has been committed, it will remain committed.

### Explore analytical data processing

Analytical data processing typically uses read-only (or read-mostly) systems that store vast volumes of historical data or business metrics. Analytics can be based on a snapshot of the data at a given point in time, or a series of snapshots. It kinda looks like this:

![Cool picture](Pictures/analytical-processing.png)

A brief explanation of the picture above:

1. Data files may be stored in a central data lake for analysis.
2. An *extract, transform, and load (ETL)* process copies data from files and OLTP databases into a data warehouse that is optimized for read activity. Commonly, a data warehouse schema is based on fact tables that contain numeric values you want to analyze (for example, sales amounts), with related dimension tables that represent the entities by which you want to measure them (for example, customer or product),
3. Data in the data warehouse may be aggregated and loaded into an online analytical processing (OLAP) model, or cube. Aggregated numeric values (measures) from fact tables are calculated for intersections of dimensions from dimension tables. For example, sales revenue might be totaled by date, customer, and product.
4. The data in the data lake, data warehouse, and analytical model can be queried to produce reports, visualizations, and dashboards.

Some terms to remember:

- *Data lakes*. Place where a large volume of file-based data must be collected and analyzed.
- *Data warehouses*. They are an established way to store data in a relational schema that is optimized for read operations â€“ primarily queries to support reporting and data visualization. The data warehouse schema may require some denormalization of data in an OLTP data source (introducing some duplication to make queries perform faster).
- *OLAP models*. An aggregated type of data storage that is optimized for analytical workloads. Data aggregations are across dimensions at different levels, enabling you to drill up/down to view aggregations at multiple hierarchical levels; for example to find total sales by region, by city, or for an individual address. Because OLAP data is pre-aggregated, queries to return the summaries it contains can be run quickly.

## Explore data roles and services

There are various roles that organizations often apply to data professionals, the tasks and responsibilities associated with these roles, and the Microsoft Azure services used to perform them.

### Explore job roles in the world of data

There are three key job roles for data within most organizations:

- **Database administrators** manage databases, assigning permissions to users, storing backup copies of data, managing security of the data itself and restore data in the event of a failure.
- **Data engineers** manage infrastructure and processes for data integration across the organization, applying data cleaning routines, identifying data governance rules, and implementing pipelines to transfer and transform data between systems.
- **Data analysts** explore and analyze data to create visualizations and charts that enable organizations to make informed decisions.

### Identify data services

There are several data services that can be used:

- **Azure SQL** is the collective name for a family of relational database solutions based on the Microsoft SQL Server database engine. Specific Azure SQL services include:
  - *Azure SQL Database*. Platform-as-a-service (PaaS) database.
  - *Azure SQL Managed Instance*. A hosted instance which allows more flexibility, but more administrative responsibility.
  - *Azure SQL VM*. A virtual machine with full configuration responsibility.
- **Azure databases** are managed services for popular open-source relational database systems, including:
  - *Azure Database for MySQL*.
  - *Azure Database for MariaDB*.
  - *Azure Database for PostgreSQL*.
- **Azure Cosmos DB** is a global-scale non-relational (NoSQL) database system that supports multiple application programming interfaces (APIs), enabling you to store and manage data as JSON documents, key-value pairs, column-families, and graphs.
- **Azure Storage** is a core Azure service that enables you to store data in:
  - *Blob containers*. Scalable, cost-effective storage for binary files.
  - *File shares*. Network file shares.
  - *Tables*. Key-value storage.
- **Azure Data Factory** is an Azure service that enables you to define and schedule data pipelines to transfer and transform data. Useful for extract, transform and load (ETL) that populate analytical data stores with data from transactional systems across the organization.
- **Azure Synapse Analytics** is a comprehensive, unified data analytics solution that provides a single service interface for multiple analytical capabilities, including:
  - *Pipelines*. Based on Azure Data Factory.
  - *SQL*. A SQL database engine optimized for data warehouse workloads.
  - *Apache Spark*. Open-source distributed data processing system that supports multiple programming languages and APIs
  - *Azure Synapse Data Explorer*. A high-performance data analytics solution that is optimized for real-time querying of log and telemetry data using Kusto Query Language (KQL).
- **Azure Databricks** is an Azure-integrated version of the popular Databricks platform, which combines the Apache Spark data processing platform with SQL database semantics and an integrated management interface to enable large-scale data analytics.
- **Azure HDInsight** is an Azure service that provides Azure-hosted clusters for popular Apache open-source big data processing technologies, including:
  - *Apache Spark*. Distributed data processing system that supports multiple programming languages and APIs.
  - *Apache Hadoop*. A distributed system that uses MapReduce jobs to process large volumes of data efficiently across multiple cluster nodes.
  - *Apache HBase*. An open-source system for large-scale NoSQL data storage and querying.
  - *Apache Kafka*. A message broker for data stream processing.
  - *Apache Storm*. An open-source system for real-time data processing through a topology of spouts and bolts.
- **Azure Stream Analytics**  is a real-time stream processing engine that captures a stream of data from an input, applies a query to extract and manipulate data from the input stream, and writes the results to an output for analysis or further processing.
- **Azure Data Explorer** is a standalone service that offers the same high-performance querying of log and telemetry data as the Azure Synapse Data Explorer runtime in Azure Synapse Analytics.
- **Azure Purview** provides a solution for enterprise-wide data governance and discoverability. You can use Azure Purview to create a map of your data and track data lineage across multiple data sources and systems, enabling you to find trustworthy data for analysis and reporting.
- **Microsoft Power BI** is a platform for analytical data modeling and reporting that data analysts can use to create and share interactive data visualizations.
